<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Grit & Grid Search</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2019-04-02T17:39:23-07:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Andrew Portal</name>
   <email>andrewjportal@gmail.com</email>
 </author>

 
 <entry>
   <title>Skytron</title>
   <link href="http://localhost:4000/2019/03/25/Skytron/"/>
   <updated>2019-03-25T00:00:00-07:00</updated>
   <id>http://localhost:4000/2019/03/25/Skytron</id>
   <content type="html">&lt;hr /&gt;
&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Skytron was built to detect anomalous flight paths in the Open Sky Network. However, it has a general use as a pipeline to analyze data as a stream and to train a model as a stream.&lt;/p&gt;

&lt;h2 id=&quot;stream-vs-batch&quot;&gt;Stream Vs Batch&lt;/h2&gt;
&lt;p&gt;Anomaly detection is best done in real time, as stream. If there is problem, the more time passes the greater the potential cost to fix. If you already have data as stream, then train the model as stream for the same consideration, timeliness. Rather than wait for a sufficiently sized batch, just hit the ground running a train of each piece of data as it comes in. This also all saves on storage, because rather than train on singular large batch, the model trains as loop continuously overwriting a small piece of data.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Just to be clear, lack of historical data is a premise here. If you already have access to plenty of data then you don’t have wait to train on batch and the value of stream learning is for nought.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;training-loop&quot;&gt;Training Loop&lt;/h2&gt;
&lt;p&gt;The training loop makes an API call to the Open Sky Network collecting a flight vector (Longitude, Latitude, Altitude, Velocity, Heading) for ever flight in the network, waits 10 seconds and collect a second set of vectors, then trains on the two set; then predicts off the second set and overwrites the first for new set, evaluates prediction against new set, log errors, overwrites second set with newer set and repeats by train on the two new sets. In case this process was not clear, see the diagram below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/stream-learn-process.png&quot; alt=&quot;Image test&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;threading&quot;&gt;Threading&lt;/h2&gt;
&lt;p&gt;While the training loop runs, I want extract information from model, such view current anomalies, extract a dictionary of all anomalies detected and the trained model once it has stabilized. Also, I want to  display a streaming set of plots for model evaluation. Threading enables these process to run in tandem on the same set of variables.  Threading is native to python, knowing its existence was key, but it’s implementation trivial. If needed, please review this technical diagram I made.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/tech-d.png&quot; alt=&quot;Image test&quot; /&gt;
&lt;em&gt;-please note the little plane vectors&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;
&lt;p&gt;The general framework of the model is simple. Each piece is modular. For example, I could use a tensor instead a vector and convolutional neural net instead of a multilayer perceptron to detect anomalous video streams instead of flight paths. The framework remains, so the underlying machinery is the same. One thing that shouldn’t change is the use of some kind of neural net. Otherwise, we need a function comparing  deviation in say latitude compared to velocity. A neural net saves us from this, because it will not optimize for one variable over another. Genearly, all the errors will be on a similar scale so that we can reasonably just take an average.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/model.png&quot; alt=&quot;Image test&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;streaming-evaluation-plots&quot;&gt;Streaming Evaluation Plots&lt;/h2&gt;
&lt;p&gt;With streaming data there is no classic train test split, because were are not waiting around for batches to accumulate and even more problematic we don’t have data labels. &lt;em&gt;That’s right folks, this is unsupervised learning.&lt;/em&gt;(Kinda)**  I found the only option for validation is simply the eye test. I created two live streaming and just judge if the results intuitively made sense. Below, at the early stage the model is unstable, high error, thus I am not confident. Later, the model stabilizes and there is less variance in the anomalies detected, so I have more confidence. The results still do not make perfect sense, there were not 1,000 anomalous flight, at least in a meaningful sense. This could be artifact of the anomaly criterion being just one standard deviation. If were to check with three, the result might be more line with what I am looking for.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/plots.png&quot; alt=&quot;Image test&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Add the ability to run multiple models at once, select for the best performing model.&lt;/li&gt;
  &lt;li&gt;Be able tune anomaly criteria, in certain cases we might want to catch more or fewer anomalies&lt;/li&gt;
  &lt;li&gt;Test on a different kind of data stream with a different neural net. Ex. Golf swing video&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>YouTopics</title>
   <link href="http://localhost:4000/2019/03/08/YouTopics/"/>
   <updated>2019-03-08T00:00:00-08:00</updated>
   <id>http://localhost:4000/2019/03/08/YouTopics</id>
   <content type="html">&lt;hr /&gt;
&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;YouTopics allows the user to quickly analyze the relevance of a youtube video.
Given a user-generated list of keywords representing a topic,
YouTopics measures the similarity between the user’s topic
and a youtube video’s topic.&lt;/p&gt;

&lt;h3 id=&quot;motivation&quot;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;YouTube has a lot information that can be analyzed with NLP thanks to
transcripts and auto generated captions. Despite its popularity, I was unable
to find example projects working with this information and so it appears youtube is an
untouch trove of human communication waiting to be processed.&lt;/p&gt;

&lt;h3 id=&quot;final-product&quot;&gt;Final Product&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/youtopics.png&quot; alt=&quot;Image test&quot; /&gt;
 YouTopics was built to be a web app, but the word embeddings file is too large to be hosted by services like Heroku and Google’s app engine. I can host it by opening up an aws server. If I end up doing so a link will posted here.&lt;/p&gt;

&lt;h3 id=&quot;approach&quot;&gt;Approach&lt;/h3&gt;
&lt;p&gt;My grand idea is to extract a summary of the transcripts or captions and then
provide an appropriate topic label. The go to topic models are LDA - and SVD -.&lt;/p&gt;

&lt;h4 id=&quot;lda&quot;&gt;LDA:&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;LDA takes a collection of documents and with an assumed number of topics, assumed distribution of topics across documents and assumed distribution of words across topics, the model will output, based on probability, a collection of words representing the assumed topics. Intuitively, given the high variance of topics and distributions across billions of videos, LDA won’t generalize well. (Or as dad would say, assume = ass + u + me)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;svd&quot;&gt;SVD&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;I actually really like SVD, which is also known as the fundamental theorem of linear algebra. It’s the base technique for  many analytical methods. When applied to NLP it is often referred to as LSA-Latent Semantic Analysis.(There’s too many acronyms in this world). SVD takes a matrix, in this case a term by document matrix, and decomposes it into three matrices, a term by topics, topic by topic and topic by doc matrix. Like LDA, I fear the topics extracted will not be generally applicable.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;My goal is to analyze individual videos, so I will need a unique approach from the  LDA / SVD muti-doc one.
Considering the outcome of LDA and SVD is just a list of keywords representing a topic and the python library Gensim has a built in TextRank algorithm that extracts keywords for a given document. I can use these keywords to represent topics and I can then assign a label based on the keywords similarity with pre-established topic. I can use pre-trained word vectors to compute this similarity.&lt;/p&gt;

&lt;h4 id=&quot;word-vectors&quot;&gt;Word Vectors&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;Word2Vec and GLoVe word embedding map words to vector space where the distance between words is related to semantic similarity. The semantic similarity is computed with the Word2Vec by taking the parameters from a neural network prediction the surrounding words. GloVe uses a more direct approach, GloVe’s objective is to predict the log probability of word occurrence across the whole corpus. GloVe’s approach is more complicate, but produces generally better results than Word2Vec. Since, I am going to use Stanford’s pretrained model the complexity is not a concern, and so I will GloVe embeddings for the performance.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I have determined a method to extract keywords and compare them. The last step is to extract a summary.
NLP summary methods are divided into extractive and abstractive methods. Abstraction uses a lot of compute power to make new summary sentences. I prefer extracting important sentences from the document. It’s effective and conveys the same meaning.This is subjective but objective evaluation of NLP is a whole other problem. Gensim has a inbuilt summarizer based on BM25 along with TextRank.&lt;/p&gt;

&lt;h4 id=&quot;textrank&quot;&gt;TextRank&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;Is the same algorithm as PageRank, assigns rank based on unique number of links. In TextRank, links are words in immediate proximity.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;bm25&quot;&gt;BM25&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;Ranks sentences based on inverse frequency and frequency of keywords from TextRank. BM25
is still obscure to me but it seems intuitively linked to the idea information entropy. A sentence value is based not just on frequency of keywords, but the information value the keywords have in the sentence. Regardless, the results pass the eye test.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The next step is to extract the transcripts from youtube. This can be done through the youtube API. There is always a kind soul in the python communities who will implement a python version of popular API’s. In this case I found PyTube.&lt;/p&gt;

&lt;h4 id=&quot;pytube&quot;&gt;PyTube&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;Works great. Thanks, stranger.&lt;a href=&quot;https://github.com/nficano/pytube&quot;&gt;Link to the repo&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;future-work&quot;&gt;Future Work&lt;/h3&gt;
&lt;p&gt;Add a database storage function for user defined topics. Then apply
database of topics for more robust topic analysis. Add multi-topic
analysis functionality.&lt;/p&gt;

</content>
 </entry>
 

</feed>
