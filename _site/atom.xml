<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Grit & Grid Search</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2019-04-08T14:08:44-07:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Andrew Portal</name>
   <email>andrewjportal@gmail.com</email>
 </author>

 
 <entry>
   <title>Skytron</title>
   <link href="http://localhost:4000/2019/03/25/Skytron/"/>
   <updated>2019-03-25T00:00:00-07:00</updated>
   <id>http://localhost:4000/2019/03/25/Skytron</id>
   <content type="html">&lt;hr /&gt;
&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Skytron was built to detect anomalous flight paths in the Open Sky Network. However, it has a general use as a pipeline to analyze data as a stream and to train a model as a stream.&lt;/p&gt;

&lt;h2 id=&quot;stream-vs-batch&quot;&gt;Stream Vs Batch&lt;/h2&gt;
&lt;p&gt;Anomaly detection is best done in real time, as stream. If there is problem, the more time passes the greater the potential cost to fix. If you already have data as stream, then train the model as stream for the same consideration, timeliness. Rather than wait for a sufficiently sized batch, just hit the ground running a train of each piece of data as it comes in. This also all saves on storage, because rather than train on singular large batch, the model trains as loop continuously overwriting a small piece of data.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Just to be clear, lack of historical data is a premise here. If you already have access to plenty of data then you don’t have wait to train on batch and the value of stream learning is for nought.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;training-loop&quot;&gt;Training Loop&lt;/h2&gt;
&lt;p&gt;The training loop makes an API call to the Open Sky Network collecting a flight vector (Longitude, Latitude, Altitude, Velocity, Heading) for ever flight in the network, waits 10 seconds and collect a second set of vectors, then trains on the two set; then predicts off the second set and overwrites the first for new set, evaluates prediction against new set, log errors, overwrites second set with newer set and repeats by train on the two new sets. In case this process was not clear, see the diagram below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/stream-learn-process.png&quot; alt=&quot;Image test&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;threading&quot;&gt;Threading&lt;/h2&gt;
&lt;p&gt;While the training loop runs, I want extract information from model, such view current anomalies, extract a dictionary of all anomalies detected and the trained model once it has stabilized. Also, I want to  display a streaming set of plots for model evaluation. Threading enables these process to run in tandem on the same set of variables.  Threading is native to python, knowing its existence was key, but it’s implementation trivial. If needed, please review this technical diagram I made.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/tech-d.png&quot; alt=&quot;Image test&quot; /&gt;
&lt;em&gt;-please note the little plane vectors&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;
&lt;p&gt;The general framework of the model is simple. Each piece is modular. For example, I could use a tensor instead a vector and convolutional neural net instead of a multilayer perceptron to detect anomalous video streams instead of flight paths. The framework remains, so the underlying machinery is the same. One thing that shouldn’t change is the use of some kind of neural net. Otherwise, we need a function comparing  deviation in say latitude compared to velocity. A neural net saves us from this, because it will not optimize for one variable over another. Genearly, all the errors will be on a similar scale so that we can reasonably just take an average.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/model.png&quot; alt=&quot;Image test&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;streaming-evaluation-plots&quot;&gt;Streaming Evaluation Plots&lt;/h2&gt;
&lt;p&gt;With streaming data there is no classic train test split, because were are not waiting around for batches to accumulate and even more problematic we don’t have data labels. &lt;em&gt;That’s right folks, this is unsupervised learning.&lt;/em&gt;(Kinda)**  I found the only option for validation is simply the eye test. I created two live streaming and just judge if the results intuitively made sense. Below, at the early stage the model is unstable, high error, thus I am not confident. Later, the model stabilizes and there is less variance in the anomalies detected, so I have more confidence. The results still do not make perfect sense, there were not 1,000 anomalous flight, at least in a meaningful sense. This could be artifact of the anomaly criterion being just one standard deviation. If were to check with three, the result might be more line with what I am looking for.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/plots2.png&quot; alt=&quot;Image test&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Add the ability to run multiple models at once, select for the best performing model.&lt;/li&gt;
  &lt;li&gt;Be able tune anomaly criteria, in certain cases we might want to catch more or fewer anomalies&lt;/li&gt;
  &lt;li&gt;Test on a different kind of data stream with a different neural net. Ex. Golf swing video&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>YouTopics</title>
   <link href="http://localhost:4000/2019/03/08/YouTopics/"/>
   <updated>2019-03-08T00:00:00-08:00</updated>
   <id>http://localhost:4000/2019/03/08/YouTopics</id>
   <content type="html">&lt;hr /&gt;
&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;YouTopics allows the user to quickly analyze the relevance of a youtube video.
Given a user-generated list of keywords representing a topic,
YouTopics measures the similarity between the user’s topic
and a youtube video’s topic.&lt;/p&gt;

&lt;h3 id=&quot;motivation&quot;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;YouTube has a lot information that can be analyzed with NLP thanks to
transcripts and auto generated captions. Despite its popularity, I was unable
to find example projects working with this information and so it appears youtube is an
untouch trove of human communication waiting to be processed.&lt;/p&gt;

&lt;h3 id=&quot;final-product&quot;&gt;Final Product&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/youtopics.png&quot; alt=&quot;Image test&quot; /&gt;
 YouTopics was built to be a web app, but the word embeddings file is too large to be hosted by services like Heroku and Google’s app engine. I can host it by opening up an aws server. If I end up doing so a link will posted here.&lt;/p&gt;

&lt;h3 id=&quot;approach&quot;&gt;Approach&lt;/h3&gt;
&lt;p&gt;My grand idea is to extract a summary of the transcripts or captions and then
provide an appropriate topic label. The go to topic models are LDA - and SVD -.&lt;/p&gt;

&lt;h4 id=&quot;lda&quot;&gt;LDA:&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;LDA takes a collection of documents and with an assumed number of topics, assumed distribution of topics across documents and assumed distribution of words across topics, the model will output, based on probability, a collection of words representing the assumed topics. Intuitively, given the high variance of topics and distributions across billions of videos, LDA won’t generalize well. (Or as dad would say, assume = ass + u + me)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;svd&quot;&gt;SVD&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;I actually really like SVD, which is also known as the fundamental theorem of linear algebra. It’s the base technique for  many analytical methods. When applied to NLP it is often referred to as LSA-Latent Semantic Analysis.(There’s too many acronyms in this world). SVD takes a matrix, in this case a term by document matrix, and decomposes it into three matrices, a term by topics, topic by topic and topic by doc matrix. Like LDA, I fear the topics extracted will not be generally applicable.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;My goal is to analyze individual videos, so I will need a unique approach from the  LDA / SVD muti-doc one.
Considering the outcome of LDA and SVD is just a list of keywords representing a topic and the python library Gensim has a built in TextRank algorithm that extracts keywords for a given document. I can use these keywords to represent topics and I can then assign a label based on the keywords similarity with pre-established topic. I can use pre-trained word vectors to compute this similarity.&lt;/p&gt;

&lt;h4 id=&quot;word-vectors&quot;&gt;Word Vectors&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;Word2Vec and GLoVe word embedding map words to vector space where the distance between words is related to semantic similarity. The semantic similarity is computed with the Word2Vec by taking the parameters from a neural network prediction the surrounding words. GloVe uses a more direct approach, GloVe’s objective is to predict the log probability of word occurrence across the whole corpus. GloVe’s approach is more complicate, but produces generally better results than Word2Vec. Since, I am going to use Stanford’s pretrained model the complexity is not a concern, and so I will GloVe embeddings for the performance.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I have determined a method to extract keywords and compare them. The last step is to extract a summary.
NLP summary methods are divided into extractive and abstractive methods. Abstraction uses a lot of compute power to make new summary sentences. I prefer extracting important sentences from the document. It’s effective and conveys the same meaning.This is subjective but objective evaluation of NLP is a whole other problem. Gensim has a inbuilt summarizer based on BM25 along with TextRank.&lt;/p&gt;

&lt;h4 id=&quot;textrank&quot;&gt;TextRank&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;Is the same algorithm as PageRank, assigns rank based on unique number of links. In TextRank, links are words in immediate proximity.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;bm25&quot;&gt;BM25&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;Ranks sentences based on inverse frequency and frequency of keywords from TextRank. BM25
is still obscure to me but it seems intuitively linked to the idea information entropy. A sentence value is based not just on frequency of keywords, but the information value the keywords have in the sentence. Regardless, the results pass the eye test.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The next step is to extract the transcripts from youtube. This can be done through the youtube API. There is always a kind soul in the python communities who will implement a python version of popular API’s. In this case I found PyTube.&lt;/p&gt;

&lt;h4 id=&quot;pytube&quot;&gt;PyTube&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;Works great. Thanks, stranger.&lt;a href=&quot;https://github.com/nficano/pytube&quot;&gt;Link to the repo&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;future-work&quot;&gt;Future Work&lt;/h3&gt;
&lt;p&gt;Add a database storage function for user defined topics. Then apply
database of topics for more robust topic analysis. Add multi-topic
analysis functionality.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Catboosterization</title>
   <link href="http://localhost:4000/2019/02/22/Catboosterization/"/>
   <updated>2019-02-22T00:00:00-08:00</updated>
   <id>http://localhost:4000/2019/02/22/Catboosterization</id>
   <content type="html">&lt;hr /&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/Nyan.jpeg&quot; alt=&quot;Image test&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;This project uses data from Kaggle’s Amazon.com - Employee Access Challenge and applies the Catboost algorithm to predict employee resource access based on role attributes. These role attributes are all categorical. Chartboost is a boosted tree algorithm similar to XGBoost, designed to work well with largely categorical data.&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Categorical data is not rare. Everyday we make decision based on categorical heuristics. An example is food, yellow bananas are good … brown not so much. However, generally machine learning models perform better with numeric values and so categorical data is often ignored. Having Catboost in our toolbox expands the data we can work with and ultimately expands what can be accomplished through Data Science.&lt;/p&gt;

&lt;h2 id=&quot;why-not-xgboost&quot;&gt;Why not XGBoost&lt;/h2&gt;
&lt;p&gt;XGBoost, as it is implemented in sklearn, automatically one hot encoding categorical data. This a problem when a feature contains thousands of unique categories. One hot encoding takes each unique categories (minus one) and turns it into a feature with a binary value of zero or one. Large number of features can generally lead to a problem knowns as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Curse_of_dimensionality&quot;&gt;curse of dimensionality&lt;/a&gt;. Specifically tree based algorithms need select features to be roots and some branches. It can go through every possible combination, which prohibitivly expensive. Or it takes a random subset, which in this case unlikely select anything of value.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/Unique_cat.png&quot; alt=&quot;Image test&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;enter-catboost&quot;&gt;Enter Catboost&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://catboost.ai&quot;&gt;Catboost&lt;/a&gt; was developed and open sourced by Yandex. Instead of one-hot encoding, it encodes with the formula (countInclass+prior)/(total count+1).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;CountInclass = a cumulative count of assigned class for a combination of features. In my case, the classes are approved or rejected. CounInclass= the number of times this combination was approved or rejected. Total count= count for given category. Prior is value assigned based parameters for the model and i think exists to avoid zero values. There is more involved and don’t find the documentation to be very clear,but if you wish learn more, here is a [link] (https://catboost.ai/docs/concepts/algorithm-main-stages_cat-to-numberic.html)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h3&gt;
&lt;p&gt;The data set is imbalanced with approvals to rejections at 16 to 1. Accuracy would be a poor evaluation metric (predicting approval everytime score 94%) and since the Kaggle competition used Roc-Auc score, I will as well.&lt;/p&gt;
&lt;h4 id=&quot;roc-auc-score&quot;&gt;Roc-Auc Score&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;In place of auccary, the metrics of precision and recall are used.  They measure false positives and false negative respectively (TP/(TP+FP), TP/(TP+FN) ). There is a natural tradeoff between these metrics. If you want minimize false positive, assign everything to the negative class and vice versa. ROC curve(receiver operating characteristic curve) plots this trade off. Then compute the AUC (area under curve) and you have the Roc-Auc Score, which measures the overall model performance across precision and recall.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The best submissions scored around .92 Roc-Auc score. If the model always predicted approve, the Roc-Auc would be .85. To evaluate Catboost, a score around .92 would be very good and .85 would poor.&lt;/p&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;
&lt;p&gt;Right out of the box, Catboost had ROC AUC score of around .89.  I know we can do better. Yet performing a grid search did not produce higher scores. Note this grid search was not exhaustive. Catboost,  has a lot of machinery under the hood that will select for optimal parameters. A full grid search across all parameters would likely be redundant and I believe why I couldn’t break the .89 threshold. However, I noticed diversity in the confusion matrices. This indicated the models were producing different results despite similar scores. This indicated potential for assembling.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Ensembling basically means combining models together. Models with significant diversity tend to produce better results, the intuition being variances and biases of individuals models are reduced.  Models can be assembled via a voting system, max voting, average voting and weighted voting.  Another method is stacking, which takes the output of one model feeds into another. Boosted models are already stacked. Catboost’s algorithm will continuously add trees until model performance stops improving, so I don’t think there is much to gain with additional stacking. Assembling via voting is the path forward here.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/ensemble.png&quot; alt=&quot;Image test&quot; /&gt;
I selected three models with similar overall Roc-Auc score but variances in accuracy and precision. I ensembled them using max voting. The overall improvement was only .002, but kaggle competition are won by building  very large ensembles to gain small incremental score improvements. Thus, I suspect ensembling more models would be an effective path forward to improve score. The hard will finding models with sufficient diversity.&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Build a grid search for model diversity as well as high scores threshold for better for ensembling&lt;/li&gt;
  &lt;li&gt;Try tree based anomaly detection algorithm like Isolation Forest. The class imbalance makes this task an anomaly detection one. However prebuilt implementations of Isolation Forest use one hot encoding. A custom built implementation with Catboost’s encoding might be very effective&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 

</feed>
