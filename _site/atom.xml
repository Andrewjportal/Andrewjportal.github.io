<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Grit & Grid Search</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2019-05-01T13:44:21-07:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Andrew Portal</name>
   <email>andrewjportal@gmail.com</email>
 </author>

 
 <entry>
   <title>DecadeStyles</title>
   <link href="http://localhost:4000/2019/04/30/DecadeStyles/"/>
   <updated>2019-04-30T00:00:00-07:00</updated>
   <id>http://localhost:4000/2019/04/30/DecadeStyles</id>
   <content type="html">&lt;hr /&gt;
&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;For this task, I wanted to build a clothing style classifier by decade. I could not find an existing dataset labeling attire style by decade, so I built one manually from Google images. Once this process was complete, I used transfer learning on a pre-trained deep-learning model to build my classifier.&lt;/p&gt;

&lt;p&gt;Link to application:&lt;a href=&quot;http://3.14.148.127:5000/DecadeStyles&quot;&gt;DecadeStyles&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;data-collection&quot;&gt;Data Collection&lt;/h2&gt;
&lt;p&gt;For this task, I wanted to build a clothing style classifier by decade. I could not find an existing dataset labeling attire style by decade, so I built one manually from Google images. Once this process was complete, I used transfer learning on a pre-trained deep-learning model to build my classifier.&lt;/p&gt;

&lt;h2 id=&quot;machine-learning&quot;&gt;Machine Learning&lt;/h2&gt;
&lt;p&gt;Since I was building an image classifier without tens of thousands of images, I used transfer learning. Transfer learning works off of a pre-trained model and replaces the final classification layers so as to apply it to a new set of images. Selecting the pre-trained model and structure of the final layers are hyperparameters to optimize on. In this case, I selected the VGG16 model with final layers of flatten, dropout (.75), and finally a dense layer (5, activation=softmax). The dropout layer was added to combat model overfitting. My selection process was manual, but advanced methods of Bayesian optimization and genetic algorithms can perform a more robust hyperparameter search. In this case, there was little marginal benefit of these methods because there was a cap on performance given the available data.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;The model achieved a categorical accuracy of 73% on the validation set. Accuracy is an appropriate performance metric since the categories were balanced. For more detail see the classification report below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/Classification_Report.png&quot; alt=&quot;Image test&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As this report shows, the major driver of error in the model is the 1940-1950s category, which has low recall (lots of false negatives).&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;
&lt;p&gt;In addition to building a larger dataset, focusing on specific elements of clothing could achieve better results. For example, it would be powerful to train the machine to locate pant legs and identify bell bottoms, marking the style as 70s. With enough data, an ML algorithm would learn this on its own, but directed training achieves a better result with fewer images. Training a model to identify pant legs and collars is not trivial, and with time I can build on work that has been done in this area (see DeepFashion).&lt;/p&gt;

&lt;p&gt;I could also further expand my data labels and training to other types of categories, like sub-cultures (hippie, punk) or levels of formality (semi-formal, business-casual) This model forms a nice base of work to expand on.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Skytron</title>
   <link href="http://localhost:4000/2019/03/25/Skytron/"/>
   <updated>2019-03-25T00:00:00-07:00</updated>
   <id>http://localhost:4000/2019/03/25/Skytron</id>
   <content type="html">&lt;hr /&gt;
&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Skytron was built to detect anomalous flight paths. However, it has a general use as a pipeline to analyze data as a stream and to train a model as a stream. Skytron learns to predict flight paths using the real-time data from the Open Sky Network. With the premise that anomalous flights will be harder to predict, the model logs anomalies based on prediction error.&lt;/p&gt;

&lt;h2 id=&quot;stream-vs-batch&quot;&gt;Stream Vs Batch&lt;/h2&gt;
&lt;p&gt;Skytron’s real-time anomaly detection allows immediate detection of problems and thus faster (cheaper) error correction. In the absence of historical data, rather than wait for a sufficiently sized batch you can hit the ground running and train off each piece of data as it comes in. This also saves on storage, because instead of requiring a large stored batch of training data, the model trains as a loop, continuously overwriting a small pool of data.&lt;/p&gt;

&lt;h2 id=&quot;training-loop&quot;&gt;Training Loop&lt;/h2&gt;
&lt;p&gt;The training loop makes an API call to the Open Sky Network collecting a flight vector (Longitude, Latitude, Altitude, Velocity, Heading) for every flight in the network; waits 10 seconds to collect a second set of vectors; then trains on the two sets; then predicts off the second set and overwrites the first for new set, evaluates prediction against new set, log errors, overwrites second set with newer set and repeats by training on the two new sets. To help clarify, see the diagram below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/stream-learn.png&quot; alt=&quot;Image test&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;threading&quot;&gt;Threading&lt;/h2&gt;
&lt;p&gt;While the training loop runs, I want to view current anomalies, extract a dictionary of all anomalies detected and extract the trained model once stable. I also want to display a streaming set of plots for model evaluation. Threading enables these process to run concurrently on the same set of variables. My diagram below may be helpful. Each box is an individual thread. I can extract from the Global variables in two ways…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/tech-d.png&quot; alt=&quot;Image test&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;
&lt;p&gt;Skytron’s general framework is simple and modular, so it can be applied to various applications. For example, I could use a tensor instead of a vector and convolutional neural net instead of a multilayer perceptron to detect anomalies in video streams instead of flight paths. One thing that shouldn’t change is the use of a neural net. Otherwise, we need a function comparing deviation in say latitude compared to deviation velocity. A neural net saves us from this, because its algorithm, backpropagation, will not optimize for one variable over another, so the errors will be on similar scales. We can then reasonably take an average of errors for each prediction and use it to check for anomalies.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/model.png&quot; alt=&quot;Image test&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;streaming-model-evaluation-plots&quot;&gt;Streaming Model Evaluation Plots&lt;/h2&gt;
&lt;p&gt;With streaming data, there is no classic train test split because were are not waiting around for batches to accumulate and worse we don’t have data labels. To evaluate the model, I created two live streaming plots and then judged if the results matched expectations. Below, at the early stage, the model is unstable with high error. I am not confident. Later, the model stabilizes and there is less variance in the anomalies detected. I have more confidence. The results still do not make perfect sense, there were not 1,000 anomalous flights, at least in a meaningful sense. This could be an artifact of the anomaly criterion being just one standard deviation. If I used three, the result might be more in line with what I am looking for.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/plots2.png&quot; alt=&quot;Image test&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Add the ability to run multiple models at once, select for the best performing model.&lt;/li&gt;
  &lt;li&gt;Be able to tune anomaly criteria, in certain cases we might want to catch more or fewer anomalies&lt;/li&gt;
  &lt;li&gt;Test on a different kind of data stream with a different neural net. Ex. Golf swing video&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>YouTopics</title>
   <link href="http://localhost:4000/2019/03/08/YouTopics/"/>
   <updated>2019-03-08T00:00:00-08:00</updated>
   <id>http://localhost:4000/2019/03/08/YouTopics</id>
   <content type="html">&lt;hr /&gt;
&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;YouTopics allows the user to quickly analyze the relevance of a YouTube video. Given a user-generated list of keywords representing a topic, YouTopics measures the similarity between the user’s topic and a youtube video’s topic.&lt;/p&gt;

&lt;h3 id=&quot;motivation&quot;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;YouTube has a lot of information that can be analyzed with natural language processing thanks to transcripts and auto-generated captions. Despite its popularity, I was unable to find example projects working with this information and so it appears youtube is a largely untouched trove of human communication waiting to be processed.&lt;/p&gt;

&lt;h3 id=&quot;final-product&quot;&gt;Final Product&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/youtopics.png&quot; alt=&quot;Image test&quot; /&gt;
 YouTopics was built to be a web app, but the word embeddings file is too large to be hosted by services like Heroku and Google’s app engine. I can host it by opening up an AWS server. If I end up doing so a link will be posted here.&lt;/p&gt;

&lt;h3 id=&quot;approach&quot;&gt;Approach&lt;/h3&gt;
&lt;p&gt;My idea is to extract a summary of the transcripts or captions and then provide an appropriate topic label. The go-to topic models are LDA - and SVD -.&lt;/p&gt;

&lt;h4 id=&quot;lda&quot;&gt;LDA:&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;LDA takes a collection of documents and with an assumed number of topics, assumed distribution of topics across documents and assumed distribution of words across topics, generates, based on probability, a collection of words representing topics. Intuitively, given the high variance of topics and distributions across billions of videos, LDA won’t generalize well with so many assumptions.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;svd&quot;&gt;SVD&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;I actually really like SVD, which is also known as the fundamental theorem of linear algebra. It’s the base technique for many analytical methods. When applied to NLP it is often referred to as LSA-Latent Semantic Analysis. SVD takes a matrix, in this case, a term by document matrix, and decomposes it into three matrices, a term by topics, topic by topic and topic by doc matrix. Once again, however, the number of topics is guessed or assumed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Both methods take a sample of documents to generate word lists representing topics. Finding a sample large enough to generalize these models to all of YouTube would be impractical. So, I came up with a different approach.&lt;/p&gt;

&lt;p&gt;Considering the outcome of LDA and SVD is just a list of keywords representing a topic and the python library Gensim has a built-in TextRank algorithm that extracts keywords for a given document. I used these keywords to represent topics and I then assign a label based on the similarity of the keywords with pre-established topic keywords. I  used pre-trained word vectors to compute this similarity.&lt;/p&gt;

&lt;h4 id=&quot;word-vectors&quot;&gt;Word Vectors&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;Word2Vec and GLoVe word embedding map words to vector space where the distance between words relates semantic similarity. With Word2Vec, the semantic similarity is computed by taking the parameters from a neural network prediction of surrounding words. GloVe uses a more direct approach, GloVe’s objective is to predict the log probability of word occurrence across the whole corpus. GloVe’s approach produces generally better results than Word2Vec, though it is more difficult to train. Since I am going to using pre-trained models anyway, I decided to use GloVe.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I have determined a method to extract keywords and compare them. The last step is to extract a summary.
NLP summary methods are divided into extractive and abstractive methods. Abstraction uses a lot of computing power to make new summary sentences. I prefer the extraction method, which forms summaries by just pulling important sentences from the document. It effectively conveys the same meaning while using less computing power. Gensim has an inbuilt summarizer based on BM25 along with TextRank.&lt;/p&gt;

&lt;h4 id=&quot;textrank&quot;&gt;TextRank&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;Is the same algorithm as PageRank, assigns rank based on the unique number of links. In TextRank, links are words in immediate proximity.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;bm25&quot;&gt;BM25&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;Ranks sentences based on the inverse frequency and frequency of keywords from TextRank. BM25
is still obscure to me but it seems intuitively linked to the idea information entropy. A sentence value is based not just on the frequency of keywords, but the information value the keywords have in the sentence. Regardless, the results pass the eye test.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The next step is to extract the transcripts from youtube. This can be done through the youtube API. There is always a kind soul in the python communities who will implement a python version of popular API’s. In this case, I found PyTube.&lt;/p&gt;

&lt;h4 id=&quot;pytube&quot;&gt;PyTube&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;Works great. Thanks, stranger.&lt;a href=&quot;https://github.com/nficano/pytube&quot;&gt;Link to the repo&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;future-work&quot;&gt;Future Work&lt;/h3&gt;
&lt;p&gt;Add a database storage function for user-defined topics. Then apply a database of topics for more robust topic analysis. Add multi-topic
analysis functionality.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Catboosterization</title>
   <link href="http://localhost:4000/2019/02/22/Catboosterization/"/>
   <updated>2019-02-22T00:00:00-08:00</updated>
   <id>http://localhost:4000/2019/02/22/Catboosterization</id>
   <content type="html">&lt;hr /&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/Nyan.jpeg&quot; alt=&quot;Image test&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;This project uses data from Kaggle’s Amazon.com - Employee Access Challenge and applies the Catboost algorithm to predict whether an employee will be granted access to a particular resource based on certain attributes of the employee’s role, such as manager’s id and department. These role attributes are all categorical. Catboost is a boosted tree algorithm similar to XGBoost, but designed to work well with largely categorical data.&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Generally, machine learning models perform better with purely quantitative inputs/outputs, and large amounts of categorical data is often ignored. However, categorical data is common and important for many processes and decisions (e.g. yellow bananas are good… brown not so much).&lt;/p&gt;

&lt;p&gt;Having algorithms that handle categorical data, such as XGBoost and Catboost, in the machine learning toolbox expands the data we can work with and ultimately expands what can be accomplished through Data Science.&lt;/p&gt;

&lt;h2 id=&quot;why-not-xgboost&quot;&gt;Why not XGBoost&lt;/h2&gt;
&lt;p&gt;XGBoost, as it is implemented in sklearn, automatically “one hot encodes” categorical data. One hot encoding takes each unique category (minus one) and turns it into a feature with a binary value.This is a problem when a features contains thousands of unique categories. An example is the feature RoleID, that contains thousand of different role categories.A large number of features can leads to a problem known as the curse of dimensionality; as dimensions, or features, increase data points become more distant , making it harder to discover relationships because everything is far away. Specifically tree-based algorithms must select some features to be roots and some branches. To make this determination, they can go through every possible combination, which is prohibitively expensive. Or they can take a random subset, which is  unlikely to select anything of value since most features have been encoded into  dummy variables of zero or one.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/Unique_cat.png&quot; alt=&quot;Image test&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;enter-catboost&quot;&gt;Enter Catboost&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://catboost.ai&quot;&gt;Catboost&lt;/a&gt; was developed and open-sourced by Yandex. Instead of one-hot encoding, it encodes with the formula (countInclass+prior)/(total count+1).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;CountInclass = a cumulative count of assigned class for a combination of features. In my case, the classes are approved or rejected. CounInclass= the number of times this combination was approved or rejected. 
Total count = the cumulative count for the given category. 
Prior= a value assigned based on initial parameters for the model which exists to avoid zero values.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;More information here:[link] (https://catboost.ai/docs/concepts/algorithm-main-stages_cat-to-numberic.html)&lt;/p&gt;

&lt;h3 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h3&gt;
&lt;p&gt;The data set is imbalanced with approvals to rejections at 16 to 1. Accuracy would be a poor evaluation metric, since a model that predicted approval every time would score 94% accuracy. The Roc-Auc score is a good alternative.&lt;/p&gt;

&lt;h4 id=&quot;roc-auc-score&quot;&gt;Roc-Auc Score&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;In place of accuracy, the Roc-Auc score incorporates metrics of Precision and Recall to evaluate a model. High Precision indicates low occurrence of false positives;  high Recall indicates low occurrence of false negatives. There is a natural trade-off between these metrics. The occurrence of false positives vs. false negatives can be adjusted by lowering or raising a decision threshold. The ROC curve (receiver operating characteristic curve) plots Recall vs. 1-Precision. The point (1,1) represents a threshold with perfect Recall, no false negatives, but max false positives. Computing the AUC (area under curve) produces the Roc-Auc Score, which measures the overall model performance for precision and recall, across all possible decision thresholds.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The best submissions scored around .92 Roc-Auc score. If the model always predicted approve, the Roc-Auc would be .85. To evaluate Catboost, a score around .92 would be very good and .85 would be poor.&lt;/p&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;
&lt;p&gt;When I first applied Catboost to my data, the model had a ROC-AUC score of 0.89. I attempted to increase it by performing a grid search, a loop through the model’s hyperparameters, but this did not produce higher scores. However, it did produce notable differences in the confusion matrices. This indicated the models were producing different results despite similar scores, suggesting the potential for ensembling.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Ensembling is a process of combining models together. Models with significant diversity tend to produce better results, likely because variances and biases of individual models are balanced against each other. Models can be assembled via stacking or a voting system. Stacking takes the output of one model and feeds into another. Boosted models are already stacked: Catboost’s algorithm will continuously add trees until model performance stops improving, so I don’t think there is much to gain with additional stacking.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I decided to use voting system to strengthen the model.
&lt;img src=&quot;http://localhost:4000/images/ensemble.png&quot; alt=&quot;Image test&quot; /&gt;
I selected three models with similar overall Roc-Auc scores which had variances in accuracy and precision. I ensembled them using max voting or majority voting. The overall improvement was 0.002. Kaggle competitions are often won by building very large ensembles to gain small incremental score improvements. Thus, I suspect ensembling more models would be an effective path forward to improve score, provided I can find models with sufficient diversity.&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Build a grid search for model diversity within high score threshold for better for ensembling&lt;/li&gt;
  &lt;li&gt;Try tree-based anomaly detection algorithm like Isolation Forest. The class imbalance makes this task an anomaly detection one. However, prebuilt implementations of Isolation Forest use one hot encoding. A custom built implementation with Catboost’s encoding might be very effective&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 

</feed>
